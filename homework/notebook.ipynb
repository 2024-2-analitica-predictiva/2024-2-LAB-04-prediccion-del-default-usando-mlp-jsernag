{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de datos\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "test_data = pd.read_csv(\n",
    "    \"../files/input/test_data.csv.zip\",\n",
    "    index_col = False,\n",
    "    compression = \"zip\",\n",
    ")\n",
    "\n",
    "\n",
    "train_data = pd.read_csv(\n",
    "    \"../files/input/train_data.csv.zip\",\n",
    "    index_col = False,\n",
    "    compression = \"zip\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1.\n",
    "# Realice la limpieza de los datasets:\n",
    "# - Renombre la columna \"default payment next month\" a \"default\".\n",
    "# - Remueva la columna \"ID\".\n",
    "# - Elimine los registros con informacion no disponible.\n",
    "# - Para la columna EDUCATION, valores > 4 indican niveles superiores\n",
    "#   de educación, agrupe estos valores en la categoría \"others\".\n",
    "# - Renombre la columna \"default payment next month\" a \"default\"\n",
    "# - Remueva la columna \"ID\".\n",
    "\n",
    "test_data = test_data.rename(columns={'default payment next month' : 'default'})\n",
    "\n",
    "train_data = train_data.rename(columns={'default payment next month' : 'default'})\n",
    "\n",
    "# print(test_data.columns)\n",
    "# print(train_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.drop(columns=['ID'])\n",
    "train_data = train_data.drop(columns=['ID'])\n",
    "\n",
    "# print(test_data.columns)\n",
    "# print(train_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_data = train_data.loc[train_data[\"MARRIAGE\"] != 0]\n",
    "train_data = train_data.loc[train_data[\"EDUCATION\"] != 0]\n",
    "\n",
    "test_data = test_data.loc[test_data[\"MARRIAGE\"] != 0]\n",
    "test_data = test_data.loc[test_data[\"EDUCATION\"] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['EDUCATION'] = train_data['EDUCATION'].apply(lambda x: 4 if x > 4 else x)\n",
    "test_data['EDUCATION'] = test_data['EDUCATION'].apply(lambda x: 4 if x > 4 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 2.\n",
    "# Divida los datasets en x_train, y_train, x_test, y_test.\n",
    "\n",
    "x_train = train_data.drop(columns=\"default\")\n",
    "y_train = train_data[\"default\"]\n",
    "\n",
    "x_test = test_data.drop(columns=\"default\")\n",
    "y_test = test_data[\"default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 3.\n",
    "# Cree un pipeline para el modelo de clasificación. Este pipeline debe\n",
    "# contener las siguientes capas:\n",
    "# - Transforma las variables categoricas usando el método\n",
    "#   one-hot-encoding.\n",
    "# - Descompone la matriz de entrada usando componentes principales.\n",
    "#   El pca usa todas las componentes.\n",
    "# - Escala la matriz de entrada al intervalo [0, 1].\n",
    "# - Selecciona las K columnas mas relevantes de la matrix de entrada.\n",
    "# - Ajusta una red neuronal tipo MLP.\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "categorical_features = [\"SEX\",\"EDUCATION\",\"MARRIAGE\"]\n",
    "non_categoriacal_features = x_train.columns\n",
    "non_categoriacal_features = non_categoriacal_features.drop(categorical_features).tolist()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(), categorical_features), \n",
    "        ('scaler', StandardScaler(), non_categoriacal_features)\n",
    "    ],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        ('preprocessor',preprocessor),\n",
    "        ('pca', PCA(n_components=None)),\n",
    "        ('feature_selection', SelectKBest(score_func=f_classif)),\n",
    "        ('classifier', MLPClassifier(max_iter=1000,random_state=42))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros encontrados: {'classifier__activation': 'relu', 'classifier__alpha': 0.5, 'classifier__hidden_layer_sizes': (100, 50, 100, 50, 100, 100), 'classifier__learning_rate_init': 0.0003, 'feature_selection__k': 20}\n",
      "Mejor puntuación precision train: 0.6964705882352941\n",
      "Mejor puntuación precision test: 0.6742700729927007\n"
     ]
    }
   ],
   "source": [
    "# Paso 4.\n",
    "# Optimice los hiperparametros del pipeline usando validación cruzada.\n",
    "# Use 10 splits para la validación cruzada. Use la función de precision\n",
    "# balanceada para medir la precisión del modelo.\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, balanced_accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'feature_selection__k': [20],  # Probar varios valores para la selección de características\n",
    "    'classifier__hidden_layer_sizes': [(100,50,100,50,100,100)],  # Probar varias configuraciones de capas\n",
    "    'classifier__activation': ['relu'],  # Varias funciones de activación\n",
    "    #'classifier__solver': ['sgd', 'adam', 'lbfgs'],  # Probar optimizadores diferentes\n",
    "    'classifier__alpha': [0.5],  # Valores más variados de regularización\n",
    "    #'classifier__learning_rate': ['constant'],  # Variar la tasa de aprendizaje\n",
    "    #'classifier__early_stopping': [True],\n",
    "    'classifier__learning_rate_init':[0.0003]\n",
    "    #'classifier__max_iter':[200]\n",
    "}\n",
    "\n",
    "model = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=10,\n",
    "    scoring=\"balanced_accuracy\",\n",
    "    n_jobs=-1,\n",
    "    refit = True\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "y_train_pred = model.predict(x_train)\n",
    "y_test_pred = model.predict(x_test)\n",
    "\n",
    "\n",
    "print(\"Mejores parámetros encontrados:\", model.best_params_)\n",
    "print(\"Mejor puntuación precision train:\", precision_score(y_train, y_train_pred, zero_division=0))\n",
    "print(\"Mejor puntuación precision test:\", precision_score(y_test, y_test_pred, zero_division=0))\n",
    "\n",
    "#y_pred = model.best_estimator_.predict(x_train)\n",
    "#test_score = balanced_accuracy_score(x_test, y_pred)\n",
    "#print(\"Precisión balanceada en el conjunto de prueba:\", test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 5.\n",
    "# Guarde el modelo (comprimido con gzip) como \"files/models/model.pkl.gz\".\n",
    "# Recuerde que es posible guardar el modelo comprimido usanzo la libreria gzip.\n",
    "\n",
    "import os\n",
    "import gzip\n",
    "import pickle\n",
    "\n",
    "models_dir = '../files/models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Guardar el modelo comprimido en formato gzip\n",
    "gzip_path = os.path.join(models_dir, \"model.pkl.gz\")\n",
    "with gzip.open(gzip_path, \"wb\") as gz_file:\n",
    "    pickle.dump(model, gz_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 6.\n",
    "# Calcule las metricas de precision, precision balanceada, recall,\n",
    "# y f1-score para los conjuntos de entrenamiento y prueba.\n",
    "# Guardelas en el archivo files/output/metrics.json. Cada fila\n",
    "# del archivo es un diccionario con las metricas de un modelo.\n",
    "# Este diccionario tiene un campo para indicar si es el conjunto\n",
    "# de entrenamiento o prueba. Por ejemplo:\n",
    "#\n",
    "# {'dataset': 'train', 'precision': 0.8, 'balanced_accuracy': 0.7, 'recall': 0.9, 'f1_score': 0.85}\n",
    "# {'dataset': 'test', 'precision': 0.7, 'balanced_accuracy': 0.6, 'recall': 0.8, 'f1_score': 0.75}\n",
    "#\n",
    "\n",
    "import json\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "\n",
    "def calculate_save_metrics(model, x_train, x_test, y_train, y_test):\n",
    "    y_train_pred = model.predict(x_train)\n",
    "    y_test_pred = model.predict(x_test)\n",
    "\n",
    "    metrics_train = {\n",
    "        'type': 'metrics',\n",
    "        'dataset': 'train',\n",
    "        'precision': precision_score(y_train, y_train_pred, zero_division=0),\n",
    "        'balanced_accuracy': balanced_accuracy_score(y_train, y_train_pred),\n",
    "        'recall': recall_score(y_train, y_train_pred, zero_division=0),\n",
    "        'f1_score': f1_score(y_train, y_train_pred, zero_division=0)\n",
    "    }\n",
    "\n",
    "    metrics_test = {\n",
    "        'type': 'metrics',\n",
    "        'dataset': 'test',\n",
    "        'precision': precision_score(y_test, y_test_pred, zero_division=0),\n",
    "        'balanced_accuracy': balanced_accuracy_score(y_test, y_test_pred),\n",
    "        'recall': recall_score(y_test, y_test_pred, zero_division=0),\n",
    "        'f1_score': f1_score(y_test, y_test_pred, zero_division=0)\n",
    "    }\n",
    "\n",
    "    output_dir = '../files/output'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    output_path = os.path.join(output_dir, 'metrics.json')\n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(json.dumps(metrics_train)+ '\\n')\n",
    "        f.write(json.dumps(metrics_test)+ '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 7.\n",
    "# Calcule las matrices de confusion para los conjuntos de entrenamiento y\n",
    "# prueba. Guardelas en el archivo files/output/metrics.json. Cada fila\n",
    "# del archivo es un diccionario con las metricas de un modelo.\n",
    "# de entrenamiento o prueba. Por ejemplo:\n",
    "#\n",
    "# {'type': 'cm_matrix', 'dataset': 'train', 'true_0': {\"predicted_0\": 15562, \"predicte_1\": 666}, 'true_1': {\"predicted_0\": 3333, \"predicted_1\": 1444}}\n",
    "# {'type': 'cm_matrix', 'dataset': 'test', 'true_0': {\"predicted_0\": 15562, \"predicte_1\": 650}, 'true_1': {\"predicted_0\": 2490, \"predicted_1\": 1420}}\n",
    "#\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def calculate_save_cm(model, x_train, x_test, y_train, y_test):\n",
    "    y_train_pred = model.predict(x_train)\n",
    "    y_test_pred = model.predict(x_test)\n",
    "\n",
    "    cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "    cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "    def format_cm(cm, dataset_type):\n",
    "        return {\n",
    "            'type': 'cm_matrix',\n",
    "            'dataset': dataset_type,\n",
    "            'true_0': {\n",
    "                'predicted_0': int(cm[0,0]),\n",
    "                'predicted_1': int(cm[0,1]),\n",
    "            },\n",
    "            'true_1': {\n",
    "                'predicted_0': int(cm[1,0]),\n",
    "                'predicted_1': int(cm[1,1]),\n",
    "            },\n",
    "\n",
    "        }\n",
    "    \n",
    "    metrics = [\n",
    "        format_cm(cm_train, 'train'),\n",
    "        format_cm(cm_test, 'test')\n",
    "    ]\n",
    "\n",
    "\n",
    "    output_path = '../files/output/metrics.json'\n",
    "    with open(output_path, 'a') as f:\n",
    "        for metric in metrics:\n",
    "            f.write(json.dumps(metric)+ '\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model, x_train, x_test, y_train, y_test):\n",
    "\n",
    "    import os\n",
    "    #os.makedirs('../files/output', exist_ok=True)\n",
    "\n",
    "    calculate_save_metrics(model, x_train, x_test, y_train, y_test)\n",
    "\n",
    "    calculate_save_cm(model, x_train, x_test, y_train, y_test)\n",
    "\n",
    "\n",
    "main(model, x_train, x_test, y_train, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
